{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMH723mvaCXMBSzp7Odh6sJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7FY5qS3sO2Hr","executionInfo":{"status":"ok","timestamp":1700217417527,"user_tz":-330,"elapsed":19656,"user":{"displayName":"Avijit Pal","userId":"06452329108787717528"}},"outputId":"c9c3aecf-176e-47f4-ec33-f0baa33e53a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Transformers\n","  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from Transformers) (3.13.1)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from Transformers)\n","  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from Transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from Transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from Transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from Transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Transformers) (2.31.0)\n","Collecting tokenizers<0.19,>=0.14 (from Transformers)\n","  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from Transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from Transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->Transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->Transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, Transformers\n","Successfully installed Transformers-4.35.2 huggingface-hub-0.19.4 safetensors-0.4.0 tokenizers-0.15.0\n"]}],"source":["pip install Transformers\n"]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from scipy.special import softmax\n","import numpy as np\n"],"metadata":{"id":"JykkEkjVQQQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tweet = \"@MehranShakarami today's cold @ home ğŸ˜’ https://mehranshakarami.com\"\n","tweet = 'Why hasn\\'t your team contacted me for a return pick up yet? It  been over two weeks already.your service is very disappointing.'\n","\n","# precprcess tweet\n","tweet_words = []\n","\n","for word in tweet.split(' '):\n","    if word.startswith('@') and len(word) > 1:\n","        word = '@user'\n","\n","    elif word.startswith('http'):\n","        word = \"http\"\n","    tweet_words.append(word)\n","\n","tweet_proc = \" \".join(tweet_words)\n","\n","# load model and tokenizer\n","roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n","\n","model = AutoModelForSequenceClassification.from_pretrained(roberta)\n","tokenizer = AutoTokenizer.from_pretrained(roberta)\n","\n","labels = ['Negative', 'Neutral', 'Positive']\n","\n","# sentiment analysis\n","encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n","# output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\n","output = model(**encoded_tweet)\n","\n","scores = output[0][0].detach().numpy()\n","scores = softmax(scores)\n","\n","\n","for i in range(len(scores)):\n","\n","    l = labels[i]\n","    s = scores[i]\n","    print(l,s)\n","\n","print(np.max(scores))\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wh4uhp7QXgb","executionInfo":{"status":"ok","timestamp":1700133410430,"user_tz":-330,"elapsed":2439,"user":{"displayName":"Avijit Pal","userId":"06452329108787717528"}},"outputId":"13665381-ca54-47f4-bbc1-61fb25e54ecd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Negative 0.9710317\n","Neutral 0.026007961\n","Positive 0.002960302\n","0.9710317\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","import numpy as np\n","from scipy.special import softmax\n","\n","# Load CSV file into a DataFrame\n","df = pd.read_csv('reddit_data.csv')  # Replace 'your_file.csv' with the actual CSV file path\n","\n","# Assuming the CSV file has a column named 'text' containing the tweets\n","for index, row in df.iterrows():\n","    tweet = row['comment']\n","\n","    # Preprocess tweet\n","    tweet_words = []\n","\n","    for word in tweet.split(' '):\n","        if word.startswith('@') and len(word) > 1:\n","            word = '@user'\n","        elif word.startswith('http'):\n","            word = \"http\"\n","        tweet_words.append(word)\n","\n","    tweet_proc = \" \".join(tweet_words)\n","\n","    # Load model and tokenizer\n","    roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n","    model = AutoModelForSequenceClassification.from_pretrained(roberta)\n","    tokenizer = AutoTokenizer.from_pretrained(roberta)\n","\n","    labels = ['Negative', 'Neutral', 'Positive']\n","    cumulative_scores = np.zeros(len(labels))\n","\n","    # Sentiment analysis\n","    encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n","    output = model(**encoded_tweet)\n","\n","    scores = output.logits[0].detach().numpy()\n","    scores = softmax(scores)\n","\n","\n","    # Accumulate scores\n","    cumulative_scores += scores\n","\n","  # Normalize cumulative scores to get the overall sentiment\n","    overall_scores = cumulative_scores / len(df)\n","\n","    # Print overall sentiment\n","    for i in range(len(labels)):\n","        l = labels[i]\n","        s = overall_scores[i]\n","        #print(f\"Overall {l} sentiment:\", s)\n","\n","print(\"Predicted overall sentiment:\", labels[np.argmax(overall_scores)])\n","print(\"Overall confidence:\", np.max(overall_scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Odf-tIqI2fho","executionInfo":{"status":"ok","timestamp":1700218151368,"user_tz":-330,"elapsed":50136,"user":{"displayName":"Avijit Pal","userId":"06452329108787717528"}},"outputId":"f36cda89-5c6c-42d6-f17f-f39846b7ad1b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted overall sentiment: Negative\n","Overall confidence: 0.05109739617297524\n"]}]},{"cell_type":"code","source":["import nltk"],"metadata":{"id":"CncYG-pI_Ncg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install vaderSentiment==3.1.1"],"metadata":{"id":"PgJdsNW3_dUT","executionInfo":{"status":"ok","timestamp":1700135833707,"user_tz":-330,"elapsed":11985,"user":{"displayName":"Avijit Pal","userId":"06452329108787717528"}},"outputId":"4df83131-09f2-464f-eddd-92fd09ee3d50","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vaderSentiment==3.1.1\n","  Downloading vaderSentiment-3.1.1-py2.py3-none-any.whl (149 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m841.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.1.1\n"]}]},{"cell_type":"code","source":["from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"],"metadata":{"id":"gE2Qx8WB_SFW"},"execution_count":null,"outputs":[]}]}